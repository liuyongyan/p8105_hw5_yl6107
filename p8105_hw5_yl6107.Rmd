---
title: "Homework 5 - P8105 Data Science I"
author: "Yongyan Liu (yl6107)"
date: "Oct 30, 2025"
output:
  github_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(broom)

set.seed(1)
```

## Problem 1: Birthday Paradox Simulation

### Function to simulate birthdays

Create a function that randomly assigns birthdays to n people and checks for duplicates:

```{r birthday_function}
birthday_simulation = function(n) {
  # Randomly assign birthdays to n people (1-365)
  birthdays = sample(1:365, size = n, replace = TRUE)

  # Check if there are any duplicate birthdays
  has_duplicate = any(duplicated(birthdays))

  return(has_duplicate)
}
```

### Run simulations

Run the function 10,000 times for each group size from 2 to 50:

```{r run_simulations}
# Number of simulations per group size
n_simulations = 10000

# Group sizes to test
group_sizes = 2:50

# Run simulations and calculate probabilities
simulation_results = tibble(
    group_size = group_sizes
  ) %>%
  mutate(
    # Run 10,000 simulations for each group size
    probability = map_dbl(group_size, function(n) {
      # Run the simulation n_simulations times
      results = replicate(n_simulations, birthday_simulation(n))
      # Calculate probability as proportion of TRUE results
      mean(results)
    }),
    
    # Calculate theoretical probability using the birthday paradox formula
    theoretical_prob = map_dbl(group_size, function(n) {
      if (n > 365) return(1)
      
      # P(at least one match) = 1 - P(no matches)
      prob_no_match = prod((365:(365-n+1))/365)
      1 - prob_no_match
    })
  )
```

### Visualization

Create a plot showing the probability of shared birthdays vs. group size:

```{r visualization, fig.width=8, fig.height=6}
ggplot(simulation_results, aes(x = group_size)) +
  geom_line(aes(y = theoretical_prob, color = "Theoretical"), linewidth = 2) +
  geom_line(aes(y = probability, color = "Simulation"), linewidth = 1) +
  geom_point(aes(y = probability, color = "Simulation"), size = 2, alpha = 0.6) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "gray50", linewidth = 0.5) +
  geom_vline(xintercept = 23, linetype = "dashed", color = "gray50", linewidth = 0.5) +
  annotate("text", x = 30, y = 0.5, label = "50% probability",
           vjust = -0.5, color = "gray30") +
  annotate("text", x = 23, y = 0.9, label = "n = 23",
           hjust = -0.2, color = "gray30") +
  scale_color_manual(
    name = "Method",
    values = c("Simulation" = "steelblue", "Theoretical" = "yellow")
  ) +
  scale_y_continuous(labels = scales::percent_format(),
                     breaks = seq(0, 1, 0.1)) +
  scale_x_continuous(breaks = seq(0, 50, 5)) +
  labs(
    title = "Birthday Paradox: Probability of Shared Birthdays",
    subtitle = "Comparing simulation results with theoretical probability",
    x = "Group Size (Number of People)",
    y = "Probability of At Least Two Shared Birthdays"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 10, color = "gray40"),
    panel.grid.minor = element_blank(),
    legend.position = "top"
  )
```

### Comments

The birthday paradox demonstrates a counterintuitive probability phenomenon. Key findings from the simulation:

1. **Simulation validates theory**: The simulation results (blue line) closely match the theoretical probability (yellow line) calculated using the formula P(match) = 1 - (365/365) × (364/365) × ... × ((365-n+1)/365). This confirms that our simulation is working correctly and that 10,000 iterations provide a reliable estimate.

2. **Rapid probability increase**: The probability of finding at least two people with the same birthday increases surprisingly quickly as group size grows. With just 23 people, the probability exceeds 50%.

3. **The "paradox"**: Most people intuitively expect a much larger group would be needed to reach 50% probability. Since there are 365 possible birthdays, one might initially think you'd need 100+ people for a 50% chance. However, only 23 people are needed.

4. **Near certainty with small groups**: By the time we reach 50 people, the probability is approximately `r scales::percent(simulation_results %>% filter(group_size == 50) %>% pull(probability), accuracy = 0.1)`.

## Problem 2: Power Analysis Simulation

### Simulation function

Create a function to simulate datasets and perform t-tests:

```{r simulation_function}
# Function to simulate one dataset and run t-test
sim_t_test = function(n = 30, mu = 0, sigma = 5) {
  # Generate dataset from normal distribution
  data = rnorm(n, mean = mu, sd = sigma)

  # Perform one-sample t-test (H0: mu = 0)
  test_result = t.test(data, mu = 0) %>%
    broom::tidy()
  
  # Extract estimate and p-value
  tibble(
    mu_hat = test_result$estimate,
    p_value = test_result$p.value
  )
}
```

### Run simulations for different sample mean $\mu$ values

Repeat for $\mu$ in {0, 1, 2, 3, 4, 5, 6}:

```{r sim_multiple_mu}
# Run 5000 simulations for each $\mu$ value
sim_results_all =
  tibble(true_mu = 0:6) %>%
  mutate(
    sim_data = map(true_mu, function(mu) {
      tibble(iteration = 1:5000) %>%
        mutate(
          results = map(iteration, ~sim_t_test(n = 30, mu = mu, sigma = 5))
        ) %>%
        unnest(results)
    })
  ) %>%
  unnest(sim_data)
```

### Power analysis

Calculate power (proportion of times null was rejected) for each true mu:

```{r power_analysis}
power_results =
  sim_results_all %>%
  mutate(rejected = p_value < 0.05) %>%
  group_by(true_mu) %>%
  summarize(
    power = mean(rejected),
    avg_mu_hat = mean(mu_hat),
    avg_mu_hat_rejected = mean(mu_hat[rejected])
  )
```

### Visualization 1: Power vs True mu

Plot showing the proportion of times the null was rejected:

```{r power_plot, fig.width=8, fig.height=6}
ggplot(power_results, aes(x = true_mu, y = power)) +
  geom_line(color = "steelblue", linewidth = 1.2) +
  geom_point(color = "steelblue", size = 3) +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "red", linewidth = 0.8) +
  annotate("text", x = 5, y = 0.05, label = "alpha = 0.05",
           vjust = -0.5, color = "red") +
  scale_y_continuous(labels = scales::percent_format(),
                     breaks = seq(0, 1, 0.1),
                     limits = c(0, 1)) +
  scale_x_continuous(breaks = 0:6) +
  labs(
    title = "Statistical Power vs Effect Size",
    subtitle = "One-sample t-test (n=30, sigma=5, alpha=0.05, 5000 simulations per mu)",
    x = "True mu (Effect Size)",
    y = "Power (Proportion of Times H_0 Rejected)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 10, color = "gray40"),
    panel.grid.minor = element_blank()
  )
```

### Visualization 2: Average $\mu$ vs True mu

Plot comparing average estimate across all samples vs only rejected samples:

```{r mu_estimate_plot, fig.width=8, fig.height=6}
# Prepare data for plotting
plot_data = power_results %>%
  select(true_mu, avg_mu_hat, avg_mu_hat_rejected) %>%
  pivot_longer(
    cols = c(avg_mu_hat, avg_mu_hat_rejected),
    names_to = "sample_type",
    values_to = "average_estimate"
  ) %>%
  mutate(
    sample_type = recode(sample_type,
                        "avg_mu_hat" = "All samples",
                        "avg_mu_hat_rejected" = "Rejected samples only")
  )

ggplot(plot_data, aes(x = true_mu, y = average_estimate, color = sample_type)) +
  geom_line(linewidth = 1.2) +
  geom_point(size = 3) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed",
              color = "black", linewidth = 0.8) +
  annotate("text", x = 5, y = 5.5, label = "y = x (perfect estimate)",
           color = "black", size = 3.5) +
  scale_color_manual(
    name = "Sample Type",
    values = c("All samples" = "steelblue", "Rejected samples only" = "coral")
  ) +
  scale_x_continuous(breaks = 0:6) +
  scale_y_continuous(breaks = 0:6) +
  labs(
    title = "Average Estimated mu vs True mu",
    subtitle = "Comparing all samples with samples where null was rejected",
    x = "True mu",
    y = "Average mu"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 10, color = "gray40"),
    panel.grid.minor = element_blank(),
    legend.position = "top"
  )
```

### Interpretation

**Effect size and power:**

The power plot demonstrates the fundamental relationship between effect size and statistical power:

1. **At $\mu$ = 0** (null hypothesis true): Power equals $\alpha$ ≈ 0.05, which is expected - this is the Type I error rate.

2. **Increasing power with effect size**: As the true $\mu$ increases from 0 to 6, power increases dramatically. By $\mu$ = 4, power exceeds 0.95, and by $\mu$ = 6, power approaches 1.0 (nearly 100%).

3. **Practical implication**: Larger effect sizes are easier to detect. With our sample size (n=30) and variability ($\sigma$=5), moderate effects (mu ≥ 3) can be detected with high reliability.

**Average $\mu$ comparisons:**

1. **All samples (blue line)**: The average $\mu$  across all samples closely follows the y = x line, indicating that our estimator is unbiased - on average, we correctly estimate the true $\mu$ regardless of its value.

2. **Rejected samples only (coral line)**:
   - When $\mu$ = 0, the average $\mu$  for rejected samples is noticeably higher than 0. This makes sense: when the null is true, rejections only occur due to random chance producing extreme values.
   - For small effect sizes (mu = 1, 2), the rejected-sample average still exceeds the true mu, showing **selection bias**: we're more likely to reject when random sampling gives us unusually large sample means.
   - As $\mu$ increases (mu ≥ 4), the rejected-sample averages converge toward the true values because power is so high that nearly all samples lead to rejection, eliminating selection bias.

3. **Why rejected-sample means don't equal true $\mu$ for small effects**: When power is low, the samples that lead to rejection tend to be those with sample means inflated by sampling variability. Only samples with higher sample mean cross the significance threshold, creating upward bias in the average sample means among successful rejections.

